Cleaning data (part 2)
Advanced cleaning with OpenRefine
link for slides: http://bit.ly/bu-cleaning-data-p2 
Be sure to check out these installation instructions before starting this workshop.  

First steps:
Make sure you've downloaded and installed OpenRefine:
http://openrefine.org/download.html
1) Go to the Downloads page
2) Select the appropriate application for your computer
See more information on downloading the OpenRefine software here: https://github.com/OpenRefine/OpenRefine/wiki/Installation-Instructions 



What we'll cover
- First things first (software installation check)
- Web technologies and non-rectangular data types
- Wrectangling your data
- Make it reproducible  
- Share your work!
 Many times, the data we encounter on the web (and other places) is in a format we can't directly use, so we need to make some changes to the structure to make it more useful.

The internet is full of all kinds of data types, and we want to be able to access, clean, format, restructure, visualize, and analyze all of it. OpenRefine is a tool that helps us take in a variety of data formats and clean it into something we can use.

So much data work to do...
> Today, we capture and store a massive amount of data electronically. But often, the format and structure for capturing and measuring data are not ideal for creating visualizations or performing analyses. This situation means we need to clean data (prepare them) for analysis.

What is/where are the data?
Data are everywhere we look, and they're being used to measure nearly anything we can imagine
Numbers, text, pictures, audio, videos--there's no limit to what we can capture 

  
Data get collected from apps, devices, sensors, scanned documents, the internet, and relational databases. Storage like this usually results in a variety of file formats. Before a software algorithm can go looking for answers, the data must be cleaned up and converted into a unified form that the algorithm can understand.

But 'data cleaner' and 'data janitor' aren't very sexy...
https://www.nytimes.com/2014/08/18/technology/for-big-data-scientists-hurdle-to-insights-is-janitor-work.html




Fundamentals â€“ Data File Types & Structures
As we've noted, the information we find on the web (and other places) is in a format we can't directly use. In the next few slides, we're going to present what these formats are, why people use them, and how we can convert them into something we can analyze.

Data come in a variety of file formats
1. Someone sends you data in a downloadable file 

2. You log into an interactive front-end and retrieve the data from a storage system (i.e. a MS SQL server database system)

3. The data are available via a continuous stream that's capturing web traffic (i.e. social media)

4. You access the data through an Application Programming Interface (API)

Each of these methods might create a different file type, so it's important to know the strength and weaknesses of each. A single file that is emailed to you is handy because you can always return to the original email to retrieve it. However, larger data sources require you to interact with them via a virtual machine or desktop (VMware or VirtualBox). Sometimes there is a constant stream of data coming in from users, like on a social media application, or online business. Finally, sometimes companies or organizations have a portal for accessing data called an Application Programming Interface (API).

Types of files: text files
Computer files generally belong to two broad groups, commonly called text files and binary files
Files like web pages, computer source code, and open-source programming languages are all text files
These files can opened in a text editor (Notepad, Text Edit, etc.) or via the command line
Humans and computers can read these files
- These files consist of only text characters (UTF-8 and UTF-16)

- Editing them can be difficult without an understanding of the particular format, language, or structure of the text itself. 

- Text files are our friends for lots of reasons, but the most important being that they are more accessible to our collaborators (and future selves!).

Types of files: binary files
Binary files require software to open them, and are typically not human readable
MS Word, Excel, and proprietary software files  
Executables and application installation files (.dmg or .exe)  
Media files (.png, .jpg .mp4 or .mov files)
Encryption or compression files (.zip or .rar)
Humans can't read these files!!
These files need software to open and edit, and usually can't be viewed via the command line. Most media files, database files, executables, and zipped files are binary files. 

You can view plain text files, but you probably need to know the language if you're going to edit or create these files.

Rectangular files (spreadsheets)
These are files stored in columns and rows (or variables and observations)

Rectangular files usually require spreadsheet software (Excel or OpenOffice) or relational database software
Understanding how to access, manipulate, and extract the information in rectangular files (i.e., spreadsheets) is an essential part of data work because of how ubiquitous tabular data are. An absolute must-read for data in spreadsheets is "Data Organization in Spreadsheets" by Karl Broman and Kara Woo. 

Non-rectangular files (JSON & XML)
Non-rectangular data files are commonly used on the web for storing and transferring data

JSON (JavaScript Object Notation) was created in 2002 and used for data storage and transfer

XML (extensible markup language) is slightly older technology (created in 1996), but still widely used for the same purpose
Internet technologies commonly use non-rectangular data files (like JSON and XML). JSON (pronounced Jay-SON) is the JavaScript Object Notation format, and it looks a little like a list (if you've seen these before). XML (or extensible markup language) is a data description language that's useful for sharing information between different software systems.

JSON data objects
JSON is an object notation language and stores data in objects and arrays 

Why would anyone store data this way? JSON can store data and the attributes about the data within the same object.
JSON data is a general-purpose, lightweight format, widely popular, and relatively simple. You'll find JSON used directly in JavaScript code for Web pages, and it's the file format of choice for many APIs.

JSON Data Types: null, true, false, number, string

JSON Data Containers: square brackets [ ], curly brackets { }

JSON objects (in contrast to rectangular database models) can store the set of attributes for an object within the object. JSON objects have a flexible representation because they don't confine your data to columns and rows (now you can include text, images, etc.).

However, these objects typically need to be coerced into a rectangular object before analyzing.


XML data objects
XML (eXtensible Markup Language) is a language used to encode web documents and data structures

XML is also useful for transmitting information between different software systems and through APIs

XML was designed to be "self-descriptive" and carry data in a readable format 
XML (or extensible markup language) is a data description language for storing data. It is particularly useful as a format for sharing information between different software systems. You can read more about the XML standard here on W3Schools.

This is an example of a complete XML document:

<?xml version="1.0"?> 
<temperatures>
<filename>ISCCPMonthly_avg.nc</filename>
<case date="16-JAN-1994"
temperature="278.9"/> 
</temperatures> 

Readable here means by humans and machines.
					
				
			
		






Why bother with these technologies?
These are weird formats--why do we need to spend all this time learning how to use them?

API: Application Programming Interface 
An API is the set of instructions for accessing or transmitting information between software applications
JSON and XML are usually how data are transferred in/out of APIs 
Example: I Know Where Your Cat Lives

From about, 

"I Know Where Your Cat Lives iknowwhereyourcatlives.com is a data visualization experiment that locates a sample of one million public images of cats on a world map by the latitude and longitude coordinates embedded in their metadata. The cats were accessed via publicly available APIs provided by popular photo sharing websites. The photos were then run through various clustering algorithms using a supercomputer in order to represent the enormity of the data source."


API: Application Programming Interface 
1) Photos from Flickr and Instagram get stored in an API 

2) Requests are made for certain photos with GPS data 
Example: I Know Where Your Cat Lives  

I know Where Your Cat Lives is a project that uses the APIs of Flickr and Instagram to access certain photos (i.e. photos of cats, or a specific landmark) with GPS data (in something called an `Exif` record). 


API: Application Programming Interface 
3) Combine image meta data  with GPS data from OpenStreetMap API 
Example: I Know Where Your Cat Lives  

I know Where Your Cat Lives is a project that uses the APIs of Flickr and Instagram to access certain photos (i.e. photos of cats, or a specific landmark) with GPS data (in something called an `Exif` record). 

GPS data is imported via a map application API, then these photos are placed on a map according to the metadata stored in their photo.




API: Application Programming Interface 
4) Use Google Maps to display the image on the satellite image 
Example: I Know Where Your Cat Lives  

I know Where Your Cat Lives is a project that uses the APIs of Flickr and Instagram to access certain photos (i.e. photos of cats, or a specific landmark) with GPS data (in something called an `Exif` record). 

GPS data is imported via a map application API, then these photos are placed on a map according to the metadata stored in their photo.

These images and maps are then loaded into an application that load the two at the same time.




A Quick Example

Load Address Data into OpenRefine
1. Navigate to this link and load the addresses into OpenRefine:

Raw CSV address data

2. Create a new project called us-addresses

	

 
APIs let you combine data from multiple sources, and there really is no limit to the amount of available API data. Just to list a few:

1. data.world
2. datausa.io
3. opensecrets
4. govinfo api

We will be using the OpenMapQuest API to access longitude and latitude information from a .csv file of 234 addresses in Massachusetts and New York. These addresses are in two columns, street and city_state_zip.



Create full_address column

Use the following GREL code to create a new full_address column:

value + ", " + cells["city_state_zip"].value
Click OK.
Select the little arrow on the street column and click on,

 Edit column > 
Add column based on this column
Here we're going to use the General Refine Expression Language (GREL), but OpenRefine also allows for Jython (Python implemented in Java), and Clojure (a functional language that resembles Lisp). There are quite a few resources out there for GREL expressions, but I like the one from Illinois Library. We're going to use the GREL language to prepare the address columns for mapping.

Select the little arrow on the `street` column and click on Edit column >> Add column based on this column. In the dialogue box  under 'New column name', enter the name of the new column (`full_address`). In the section for On error, select `store error`. Under the section for the Expression, enter the GREL expression below:

value + ", " + cells["city_state_zip"].value

This will give us a new column with street, city, state, and zip.




Arrange columns
Select the little arrow on the new full_address column and click on,

 Edit column > Move column to the end
Just to make things a little clearer, we'll move the new column to the end. Moving columns around gives us the ability to view columns side-by-side, and provides some visible reassurance about the data transformations we're making. 




MapQuest API
Head over to: 
https://developer.mapquest.com/
Click on the button to get your Free API Key



Head over to MapQuest and get a Free API Key. 

The instructions are fairly straightforward, but you will need to get into the developer dashboard. 



Create and Approve MapQuest API Key
1. You can create a new key or use the one listed under My Application



2. Click Manage Keys then Approve All Keys 


Head over to MapQuest and get a Free API Key. 

The instructions are fairly straightforward, but you will need to get into the developer dashboard, and your keys will be under Manage Keys

You can create a new application, or use the one given under My Application. Read more about API keys here.


APIs with OpenRefine
OpenRefine works with GET request APIs.

Example MapQuest API query 
Learning the components of API requests is an important part of accessing and gathering data. Most sites have at least a little documentation about how to access the data available on their API. When they don't, fortunately all requests have similar components and syntax. 

The parts of an API request are in the figure above. You can also read more on the documentation website here.

Example API query parameters with GREL

Click on the new full_address column and select, Add column by fetching URLs




Follow the steps in the diagram to the right, but refer to the notes below for the GREL expression:



After you've entered the Expression, check the 1st row in the Preview section at the bottom of the dialogue box.
Now we will create another column using OpenRefine's data enrichment abilities. OpenRefine gives us the ability to load various data APIs into our existing projects. 

New column name = mapquest_locations

On error = store error 

Throttle delay = 1000 milliseconds 

Expression = 'http://open.mapquestapi.com/nominatim/v1/search.php?'+ 'format=json&' + 'key=JWbcrLmu5UfJ9n1krjAMdr0jz3QIG0ha&'+ 'q='+ escape(value, "url")


Example API query parameters with GREL

This takes time!
It took me a few minutes to run this API request, so expect to wait a bit for the data to load into OpenRefine.




JSON data
When you're done, you should see the following JSON data in the new mapquest_locations column
After OpenRefine is finished loading the data into mapquest_locations, you'll see the JSON data is all crammed into a single row. This makes it very hard to see what's in this new column, so we'll need to parse the JSON data into something a little easier to work with. 



Parsing JSON data in OpenRefine
Click on the new mapquest_locations column and select Edit column > Add column based on this column...
In the dialogue box, enter the following options:
1. Name new column = latitude 
2. On error = store error
3. Expression = value.parseJson()[0].lat
3. Verify this is correct in the Preview pane

Parsing JSON data in OpenRefine is made simpler using GREL's parseJson() function. We can enter the following expression into the Expression box, 

value.parseJson()[0].lat

Then we name our new column (latitude), and select store error (this is a good practice in case there was a typo or anything that needs to be debugged). We also want to check out the contents in the Preview pane at the bottom of the dialogue window (make sure it looks like latitude data).






Repeat the same steps on the  mapquest_locations column, but this time create a longitude column
Parsing JSON data in OpenRefine
Repeating the process is exactly the same as the process above, but I'm going to change lat to lon

value.parseJson()[0].lon

Check to make sure this code is working, then click "OK".





Check out these links to learn more!
1. John R Little has a great Pragmatic Datafication workshop with some great materials (slides and workbook) 
2. Check out the OpenRefine GREL documentation on Github (lots of great tidbits in here!)
3. Good 'ol fashioned YouTube! Check out the following video to learn more: 
	- Data Journalism - Cleaning Data in Workbench and OpenRefine
- OpenRefine Beginners Tutorial
- Clean Your Data: Getting Started with OpenRefine



Check out these slides and the github page for more information!
